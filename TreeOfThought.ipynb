{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "```!pip install openai numpy tqdm tiktoken```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\$\", \"\", text)\n",
    "    text = re.sub(r\"(?s).*#### \", \"\", text)\n",
    "    text = re.sub(r\"\\.$\", \"\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    \n",
    "    if not text:\n",
    "        return \"-1000000000\"\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_value(text: str) -> str:\n",
    "    pattern = r\"(-?[$0-9.,]{2,})|(-?[0-9]+)\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    if matches:\n",
    "        for match_groups in matches[::-1]:\n",
    "            for group in match_groups:\n",
    "                if group:\n",
    "                    return clean_text(group)\n",
    "    \n",
    "    return \"-1000000000\"\n",
    "\n",
    "def load_dataset(file_path: str, sample_size: int = 20) -> List[Dict]:\n",
    "    \"\"\"Load and sample from dataset.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return random.sample(data, sample_size)\n",
    "\n",
    "def count_tokens(text: str, model: str) -> int:\n",
    "    \"\"\"Count tokens in text using tiktoken.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def evaluate_accuracy(predictions: List[str], ground_truth: List[str]) -> float:\n",
    "    \"\"\"Calculate accuracy of predictions.\"\"\"\n",
    "    correct = 0\n",
    "    for pred, truth in zip(predictions, ground_truth):\n",
    "        try:\n",
    "            if int(pred) == int(truth):\n",
    "                correct += 1\n",
    "        except:\n",
    "            pass\n",
    "    return correct / len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thoughts Implementation\n",
    "\n",
    "1. Visit [DeepInfra](https://deepinfra.com/) and **register an account**. Familiarize yourself with how to use the API by referring to the [documentation](https://deepinfra.com/docs).  \n",
    "\n",
    "2. Test the **API call** functionality provided by DeepInfra to ensure proper integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainOfThought:\n",
    "    def __init__(self, api_key: str, base_url: str = \"https://api.deepinfra.com/v1/openai\",\n",
    "                 model: str = \"Qwen/Qwen2.5-7B-Instruct\", temperature: float = 0.7):\n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        self.model = \"gpt-40\"\n",
    "        self.temperature = temperature\n",
    "        self.total_tokens = 0\n",
    "\n",
    "    def solve(self, question: str) -> Tuple[str, int]:\n",
    "        prompt = f\"\"\"Please solve this math problem step by step.\n",
    "Question: {question}\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=self.temperature,\n",
    "                max_completion_tokens=256,\n",
    "            )\n",
    "            tokens_used = response.usage.total_tokens\n",
    "            self.total_tokens += tokens_used\n",
    "            return response.choices[0].message.content, tokens_used\n",
    "        except Exception as e:\n",
    "            print(f\"Error in API call: {e}\")\n",
    "            return \"\", 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-of-Thoughts Implementation\n",
    "\n",
    "1. You are *highly encouraged* to read the [original paper](http://arxiv.org/abs/2501.02497) and run the [codebase](https://github.com/princeton-nlp/tree-of-thought-llm) first.  \n",
    "    - Otherwise, you may have no idea what ToT is doing.\n",
    "    \n",
    "    - For simplicity, start by running the basic configuration:\n",
    "        - Search algorithm: BFS\n",
    "\n",
    "        - Thought generator: propose prompt\n",
    "\n",
    "        - Task: Game of 24\n",
    "\n",
    "2. Regarding your own implementation below, feel free to experiment with various hyperparameters, including:\n",
    "    - API call parameters (e.g., `temperature`)\n",
    "    \n",
    "    - ToT implementation parameters (e.g., `max_steps`, `n_samples_per_step`)\n",
    "\n",
    "3. [Optional] You could try other **7B-level** base models instead of `Qwen2.5-7B-Instruct` in DeepInfra. You might achieve better results with proper implementation.\n",
    "\n",
    "4. [Optional] Multi-model setups are also allowedâ€”for example, using one model to generate thoughts and another to evaluate them (reward model?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeOfThoughts:\n",
    "    def __init__(self, api_key: str, base_url: str = \"https://api.deepinfra.com/v1/openai\", \n",
    "                 model: str = \"Qwen/Qwen2.5-7B-Instruct\", temperature: float = 0.6):\n",
    "        self.client = OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.total_tokens = 0\n",
    "        self.evaluation_cache = {}  # For caching evaluations\n",
    "\n",
    "    def chat_with_gpt(self, prompt: str, n: int = 1, stop: str = None) -> List[str]:\n",
    "        \"\"\"Get multiple completions from the model with proper stop sequences.\"\"\"\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=self.temperature,\n",
    "                n=n,\n",
    "                stop=[stop] if stop else None,\n",
    "                max_tokens=256\n",
    "            )\n",
    "            completions = [choice.message.content for choice in response.choices]\n",
    "            return completions\n",
    "        except Exception as e:\n",
    "            print(f\"API Error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def generate_thoughts(self, question: str, current_thought: str = \"\", n_samples: int = 3) -> List[str]:\n",
    "        prompt = (\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"So far, the reasoning is:\\n{current_thought}\\n\\n\"\n",
    "            f\"Act as a Phd in Mathematics and solve the poroblem with that outlook, Please propose the next step in the reasoning. Be concise.\"\n",
    "        )\n",
    "        # Use the prompt in chat_with_gpt to return candidate thoughts.\n",
    "        completion = self.chat_with_gpt(prompt, n=n_samples)\n",
    "        return completion\n",
    "\n",
    "    def evaluate_thought(self, question: str, thought: str, cache: bool = True) -> float:\n",
    "        \"\"\"Evaluate the quality of a thought using self-evaluation.\"\"\"\n",
    "        if cache and thought in self.evaluation_cache:\n",
    "            return self.evaluation_cache[thought]\n",
    "        \n",
    "        prompt = f'''Rate how likely this thought will lead to the correct answer (1-10):\n",
    "        Question: {question}\n",
    "        Thought: {thought}\n",
    "        Use the above Thought to Rate from 1 to 10 how likely this path is correct. so that we can move with that thought.\n",
    "        Rating (1-10): '''\n",
    "        \n",
    "        response = self.chat_with_gpt(prompt, n=1)\n",
    "        if not response: return 0.0\n",
    "        \n",
    "        try:\n",
    "            score = float(re.search(r'\\d+', response[0]).group())\n",
    "            score = max(1.0, min(10.0, score))  # Clamp to 1-10\n",
    "            if cache:\n",
    "                self.evaluation_cache[thought] = score\n",
    "            return score / 10.0  # Normalize to 0-1\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def select_best_thoughts(self, thoughts: List[str], scores: List[float], k: int = 2) -> List[str]:\n",
    "        \"\"\"Select top-k thoughts based on evaluation scores.\"\"\"\n",
    "        combined = sorted(zip(thoughts, scores), key=lambda x: x[1], reverse=True)\n",
    "        return [thought for thought, score in combined[:k]]\n",
    "\n",
    "    def solve(self, question: str, max_steps: int = 8, n_samples_per_step: int = 3, \n",
    "             k_best_thoughts: int = 2) -> str:\n",
    "        \"\"\"Solve a problem using Tree-of-Thoughts BFS.\"\"\"\n",
    "        current_thoughts = [\"\"]\n",
    "        final_answer = \"\"\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            new_thoughts = []\n",
    "            scores = []\n",
    "            \n",
    "            for thought in current_thoughts:\n",
    "                # Generate new thoughts\n",
    "                candidates = self.generate_thoughts(question, thought, n_samples_per_step)\n",
    "                # print(candidates)\n",
    "                if not candidates:\n",
    "                    continue\n",
    "                \n",
    "                # Evaluate candidates\n",
    "                candidate_scores = [self.evaluate_thought(question, f\"{thought} {c}\".strip()) \n",
    "                                   for c in candidates]\n",
    "                \n",
    "                # Select and accumulate best candidates\n",
    "                best_candidates = self.select_best_thoughts(candidates, candidate_scores, k_best_thoughts)\n",
    "                new_thoughts.extend([f\"{thought} {c}\".strip() for c in best_candidates])\n",
    "                scores.extend(candidate_scores[:k_best_thoughts])\n",
    "            \n",
    "            if not new_thoughts:\n",
    "                break\n",
    "                \n",
    "            # Select best thoughts for next iteration\n",
    "            current_thoughts = self.select_best_thoughts(new_thoughts, scores, k_best_thoughts)\n",
    "\n",
    "            # print(current_thoughts)\n",
    "            \n",
    "            # Early termination if final answer detected\n",
    "            for t in current_thoughts:\n",
    "                if \"final answer\" in t.lower():\n",
    "                    final_answer = t\n",
    "                    # print(final_answer)\n",
    "                    prompt = f\"\"\"Get the final numeric answer to the question : {question}\n",
    "                    Using the current logic: {final_answer} \n",
    "                    Important : Your Answer should end with the number which is the final answer to the question asked.\n",
    "                            \"\"\"\n",
    "                    # print(prompt)\n",
    "                    # print(\"prompt ends here\")\n",
    "                    response = self.chat_with_gpt(prompt, n=1)\n",
    "                    return final_answer.split(\"final answer\")[-1].strip() + \"final answer is\" + str(response[0])\n",
    "                \n",
    "        \n",
    "        # print(current_thoughts)\n",
    "        # print(\"current thought ends here\")\n",
    "                \n",
    "        prompt = f\"\"\"Get the final numeric answer to the question : {question}\n",
    "        Using the current logic: {str(current_thoughts[0])} \n",
    "        Important : Your Answer should end with the number which is the final answer to the question asked.\n",
    "        \"\"\"\n",
    "        # print(prompt)\n",
    "        # print(\"prompt ends here\")\n",
    "        response = self.chat_with_gpt(prompt, n=1)\n",
    "        # print(response)\n",
    "        return current_thoughts[0]+ \" answer is \" +str(response[0]) if current_thoughts else \"\"\n",
    "\n",
    "        \n",
    "        # return current_thoughts[0] if current_thoughts else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TreeOfThoughts:\n",
    "#     def __init__(self, api_key: str, base_url: str = \"https://api.deepinfra.com/v1/openai\", \n",
    "#                  model: str = \"Qwen/Qwen2.5-7B-Instruct\", temperature: float = 0.7):\n",
    "#         \"\"\"Initialize the Tree-of-Thoughts solver.\"\"\"\n",
    "#         # TODO: Initialize the OpenAI client and other necessary attributes\n",
    "#         self.client = OpenAI(base_url=base_url, api_key=api_key)\n",
    "#         self.model = model\n",
    "#         self.temperature = temperature\n",
    "#         self.evaluation_cache = {}\n",
    "        \n",
    "\n",
    "#     def chat_with_gpt(self, prompt: str, n: int = 1, stop: str = None, ) -> List[str]:\n",
    "#         \"\"\"Get completions from GPT model.\"\"\"\n",
    "#         # [IMPORTANT] `stop` is important here. You can refer to the implementation of the Game of 24 in the original ToT codebase.\n",
    "#         # TODO: Implement the chat completion function\n",
    "#         try:\n",
    "#             response = self.client.chat.completions.create(\n",
    "#                 model=self.model,\n",
    "#                 messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#                 temperature=self.temperature,\n",
    "#                 n=n,\n",
    "#                 stop=stop,\n",
    "#                 # max_tokens=max_tokens\n",
    "#             )\n",
    "#             return [choice.message.content for choice in response.choices]\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in chat_with_gpt: {e}\")\n",
    "#             return []\n",
    "        \n",
    "\n",
    "#     def generate_thoughts(self, question: str, current_thought: str = \"\", n_samples: int = 3) -> List[str]:\n",
    "#         \"\"\"Generate multiple possible next steps in reasoning.\"\"\"\n",
    "#         # [IMPORTANT] A one-shot example can help the model follow your instructions precisely.\n",
    "#         # TODO: Implement thought generation function\n",
    "#         example_prompt = \"\"\"Example:\n",
    "#             Question: If Jane has 3 apples and gives 2 to Bob, how many does she have?\n",
    "#             Current thought: \n",
    "#             Possible next steps:\n",
    "#             1. Subtract the 2 apples given away from the initial 3: 3 - 2 = 1.\n",
    "#             2. Check if there are any other apples involved; since none, the answer is 1.\n",
    "#             3. Verify by adding Bob's apples to Jane's remaining: 1 + 2 = 3, which matches the initial count.\n",
    "#             \"\"\"\n",
    "#         prompt = f\"\"\"You are a logical thinker and math problem solver. Given a question and current thought, list {n_samples} possible next steps to solve the problem. Each step must be a concise sentence. Follow the example format.\n",
    "\n",
    "#             {example_prompt}\n",
    "#             Question: {question}\n",
    "#             Current thought: {current_thought}\n",
    "#             Possible next steps:\n",
    "#             \"\"\"\n",
    "\n",
    "# #         example_prompt = '''\n",
    "# # Example:\n",
    "# # Question: If Alice has 4 books and buys 7 more, how many does she have?\n",
    "# # Current Thought: Alice started with 4 books.\n",
    "# # Possible next thoughts:\n",
    "# # 1. She adds the new books: 4 + 7\n",
    "# # 2. Check if there are any books lost: 4 - 0 + 7\n",
    "# # 3. Verify purchase quantity: 7 books purchased'''\n",
    "        \n",
    "# #         prompt = f'''{example_prompt.strip()}\n",
    "        \n",
    "# # Now generate {n_samples} possible next thoughts for:\n",
    "# # Question: {question}\n",
    "# # Current Thought: {current_thought or 'None'}\n",
    "# # Possible next thoughts (numbered 1-{n_samples}):\\n\n",
    "# # Also if you think this is the final answer mention the word 'final answer' in it'''\n",
    "        \n",
    "#         response = self.chat_with_gpt(prompt, n=1, stop=[\"\\n\\n\"])[0]\n",
    "#         thoughts = []\n",
    "#         for line in response.split('\\n'):\n",
    "#             line = line.strip()\n",
    "#             if line:\n",
    "#                 thought = re.sub(r'^[\\d\\-*]+\\s*\\.?\\s*', '', line).strip()\n",
    "#                 if thought:\n",
    "#                     thoughts.append(thought)\n",
    "#         return thoughts[:n_samples]\n",
    "\n",
    "# #     def evaluate_thought(self, question: str, thought: str, cache: bool = True) -> float:\n",
    "# #         \"\"\"Evaluate the likelihood that a thought process leads to the correct answer.\"\"\"\n",
    "# #         # [IMPORTANT] You can use the base model (Qwen2.5-7B-Instruct) for self-evaluation; however, it is not the only option.\n",
    "# #         # TODO: Implement thought evaluation function\n",
    "\n",
    "# #         eval_prompt = f\"\"\"Evaluate the quality of the following thought in solving the given problem. Provide a numerical score between 0.0 and 1.0, where 1.0 means the thought is definitely leading to the correct answer, and 0.0 means it's irrelevant or incorrect.\n",
    "\n",
    "# #             Question: {question}\n",
    "# #             Thought: {thought}\n",
    "\n",
    "# #             Your score: \"\"\"\n",
    "# # #         eval_prompt = f'''Rate how likely this thought will lead to the correct answer (0-1):\n",
    "# # # Question: {question}\n",
    "# # # Thought: {thought}\n",
    "# # # Rating (1-10): '''\n",
    "# #         response = self.chat_with_gpt(eval_prompt, n=1, stop=[\"\\n\"])[0].strip()\n",
    "# #         numbers = re.findall(r\"\\d*\\.?\\d+\", response)\n",
    "# #         if numbers:\n",
    "# #             score = float(numbers[0])\n",
    "# #             return max(0.0, min(1.0, score))\n",
    "# #         return 0.0\n",
    "\n",
    "\n",
    "#     def evaluate_thought(self, question: str, thought: str, cache: bool = True) -> float:\n",
    "#         \"\"\"Evaluate the quality of a thought using self-evaluation.\"\"\"\n",
    "#         if cache and thought in self.evaluation_cache:\n",
    "#             return self.evaluation_cache[thought]\n",
    "        \n",
    "#         prompt = f'''Rate how likely this thought will lead to the correct answer (1-10):\n",
    "# Question: {question}\n",
    "# Thought: {thought}\n",
    "# Rating (1-10): '''\n",
    "        \n",
    "#         response = self.chat_with_gpt(prompt, n=1)\n",
    "#         if not response: return 0.0\n",
    "        \n",
    "#         try:\n",
    "#             score = float(re.search(r'\\d+', response[0]).group())\n",
    "#             score = max(1.0, min(10.0, score))  # Clamp to 1-10\n",
    "#             if cache:\n",
    "#                 self.evaluation_cache[thought] = score\n",
    "#             return score / 10.0  # Normalize to 0-1\n",
    "#         except:\n",
    "#             return 0.0\n",
    "        \n",
    "#     def select_best_thoughts(self, thoughts: List[str], scores: List[float], k: int = 2) -> List[str]:\n",
    "#         \"\"\"Select the k best thoughts based on their scores.\"\"\"\n",
    "#         # TODO: Implement thought selection function\n",
    "#         if not thoughts:\n",
    "#             return []\n",
    "    \n",
    "#         combined = list(zip(thoughts, scores))\n",
    "#         combined.sort(key=lambda x: x[1], reverse=True)\n",
    "#         return [thought for thought, _ in combined[:k]]\n",
    "\n",
    "#     def solve(self, question: str, max_steps: int = 8, n_samples_per_step: int = 3, \n",
    "#              k_best_thoughts: int = 2) -> str:\n",
    "#         \"\"\"Solve a problem using Tree-of-Thoughts reasoning.\"\"\"\n",
    "#         # TODO: Implement the main solving function\n",
    "#         current_thoughts = [\"\"]\n",
    "#         for step in range(max_steps):\n",
    "#             candidates = []\n",
    "#             for thought in current_thoughts:\n",
    "#                 next_steps = self.generate_thoughts(question, thought, n_samples_per_step)\n",
    "#                 for next_step in next_steps:\n",
    "#                     new_thought = f\"{thought} {next_step}\".strip() if thought else next_step\n",
    "#                     score = self.evaluate_thought(question, new_thought)\n",
    "#                     candidates.append((new_thought, score))\n",
    "#                     print(candidates)\n",
    "#             if not candidates:\n",
    "#                 break\n",
    "#             candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "#             current_thoughts = [thought for thought, _ in candidates[:k_best_thoughts]]\n",
    "#             if candidates[0][1] >= 0.99:\n",
    "#                 prompt = f\"\"\"\n",
    "#                 Get the final numeric answer to the question : {question}\n",
    "#                 Using the current thoughts: {candidates[-1][0]} \n",
    "#                 Important : Your Answer should end with the number which is the final answer to the question asked.\n",
    "#                 \"\"\"\n",
    "#                 response = self.chat_with_gpt(prompt, n=1, stop=[\"\\n\"])\n",
    "#                 # print(response)\n",
    "#                 # print(candidates[-1][0])\n",
    "#                 return candidates[-1][0]+\" answer is \"+str(response[0])\n",
    "        \n",
    "#         prompt = f\"\"\"\n",
    "#                 Get the final numeric answer to the question : {question}\n",
    "#                 Using the current thoughts: {current_thoughts[0]} \n",
    "#                 Important : Your Answer should end with the number which is the final answer to the question asked.\n",
    "#                 \"\"\"\n",
    "#         response = self.chat_with_gpt(prompt, n=1, stop=[\"\\n\"])\n",
    "#         # print(response)\n",
    "#         return current_thoughts[0]+\" answer is \"+str(response[0]) if current_thoughts else \"\"\n",
    "#         # return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ToT Using One Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given:\n",
      "- Jean is two years older than Mark: \\( J = M + 2 \\)\n",
      "- Two years ago, Mark was 5 years older than half of Jan's age: \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- Jan is 30 years old: \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = \\frac{1}{2}(28) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Now, use \\( M = 21 \\) in the first equation to find Jean's age:\n",
      "\\[ J = M + 2 \\]\n",
      "\\[ J = 21 + 2 \\]\n",
      "\\[ J = 23 \\]\n",
      "\n",
      "Thus, Jean is 23 years old. Given:\n",
      "- \\( J = M + 2 \\)\n",
      "- \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = \\frac{1}{2}(28) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Now, use \\( M = 21 \\) in the first equation:\n",
      "\\[ J = M + 2 \\]\n",
      "\\[ J = 21 + 2 \\]\n",
      "\\[ J = 23 \\]\n",
      "\n",
      "Thus, Jean is \\(\\boxed{23}\\) years old. Given:\n",
      "- \\( J = M + 2 \\)\n",
      "- \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Using \\( M = 21 \\) in the first equation:\n",
      "\\[ J = M + 2 \\]\n",
      "\\[ J = 21 + 2 \\]\n",
      "\\[ J = 23 \\]\n",
      "\n",
      "Thus, Jean is \\(\\boxed{23}\\) years old. Given:\n",
      "- \\( J = M + 2 \\)\n",
      "- \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Thus, the next step is to use \\( M = 21 \\) in the first equation to find Jean's age:\n",
      "\\[ J = M + 2 \\]\n",
      "\\[ J = 21 + 2 \\]\n",
      "\\[ J = 23 \\]\n",
      "\n",
      "Therefore, Jean is \\(\\boxed{23}\\) years old. Given:\n",
      "- \\( J = M + 2 \\)\n",
      "- \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Thus, Jean's age \\( J \\) is:\n",
      "\\[ J = M + 2 = 21 + 2 = 23 \\]\n",
      "\n",
      "Therefore, Jean is \\(\\boxed{23}\\) years old. Given:\n",
      "- \\( J = M + 2 \\)\n",
      "- \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Thus, Jean's age \\( J \\) is:\n",
      "\\[ J = M + 2 = 21 + 2 = 23 \\]\n",
      "\n",
      "Therefore, Jean is \\(\\boxed{23}\\) years old. answer is \\(\\boxed{23}\\)\n",
      "Solution: Given:\n",
      "- Jean is two years older than Mark: \\( J = M + 2 \\)\n",
      "- Two years ago, Mark was 5 years older than half of Jan's age: \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- Jan is 30 years old: \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = \\frac{1}{2}(28) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Now, use \\( M = 21 \\) in the first equation to find Jean's age:\n",
      "\\[ J = M + 2 \\]\n",
      "\\[ J = 21 + 2 \\]\n",
      "\\[ J = 23 \\]\n",
      "\n",
      "Thus, Jean is 23 years old. Given:\n",
      "- \\( J = M + 2 \\)\n",
      "- \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = \\frac{1}{2}(28) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Now, use \\( M = 21 \\) in the first equation:\n",
      "\\[ J = M + 2 \\]\n",
      "\\[ J = 21 + 2 \\]\n",
      "\\[ J = 23 \\]\n",
      "\n",
      "Thus, Jean is \\(\\boxed{23}\\) years old. Given:\n",
      "- \\( J = M + 2 \\)\n",
      "- \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Using \\( M = 21 \\) in the first equation:\n",
      "\\[ J = M + 2 \\]\n",
      "\\[ J = 21 + 2 \\]\n",
      "\\[ J = 23 \\]\n",
      "\n",
      "Thus, Jean is \\(\\boxed{23}\\) years old. Given:\n",
      "- \\( J = M + 2 \\)\n",
      "- \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Thus, the next step is to use \\( M = 21 \\) in the first equation to find Jean's age:\n",
      "\\[ J = M + 2 \\]\n",
      "\\[ J = 21 + 2 \\]\n",
      "\\[ J = 23 \\]\n",
      "\n",
      "Therefore, Jean is \\(\\boxed{23}\\) years old. Given:\n",
      "- \\( J = M + 2 \\)\n",
      "- \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Thus, Jean's age \\( J \\) is:\n",
      "\\[ J = M + 2 = 21 + 2 = 23 \\]\n",
      "\n",
      "Therefore, Jean is \\(\\boxed{23}\\) years old. Given:\n",
      "- \\( J = M + 2 \\)\n",
      "- \\( M - 2 = \\frac{1}{2}(J - 2) + 5 \\)\n",
      "- \\( J = 30 \\)\n",
      "\n",
      "Substitute \\( J = 30 \\) into the second equation:\n",
      "\\[ M - 2 = \\frac{1}{2}(30 - 2) + 5 \\]\n",
      "\\[ M - 2 = 14 + 5 \\]\n",
      "\\[ M - 2 = 19 \\]\n",
      "\\[ M = 21 \\]\n",
      "\n",
      "Thus, Jean's age \\( J \\) is:\n",
      "\\[ J = M + 2 = 21 + 2 = 23 \\]\n",
      "\n",
      "Therefore, Jean is \\(\\boxed{23}\\) years old. answer is \\(\\boxed{23}\\)\n",
      "Extracted answer: 23\n"
     ]
    }
   ],
   "source": [
    "def test_tot():\n",
    "    # Set your API key\n",
    "    api_key = os.getenv(\"DEEPINFRA_TOKEN\", \"\")\n",
    "    if not api_key:\n",
    "        print(\"Please set DEEPINFRA_TOKEN environment variable\")\n",
    "        return\n",
    "\n",
    "    # Test with a single example\n",
    "    test_question = \"Jean is two years older than Mark.  Two years ago Mark was 5 years older than half Jan's age.  If Jan is 30 how old is Jean?\"\n",
    "    tot_solver = TreeOfThoughts(api_key)\n",
    "    solution = tot_solver.solve(\n",
    "        question=test_question,\n",
    "        max_steps=6,\n",
    "        n_samples_per_step=3,\n",
    "        k_best_thoughts=3\n",
    "    )\n",
    "    \n",
    "    print(solution)\n",
    "    answer = extract_value(solution)\n",
    "    print(\"Solution:\", solution)\n",
    "    print(\"Extracted answer:\", answer)\n",
    "\n",
    "test_tot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment on the Validation Set\n",
    "\n",
    "- You can use multithreading to accelerate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[346], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m true_answer \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# ToT solving\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m tot_solution \u001b[38;5;241m=\u001b[39m \u001b[43mtot_solver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples_per_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_best_thoughts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m tot_answer \u001b[38;5;241m=\u001b[39m extract_value(tot_solution)\n\u001b[1;32m     28\u001b[0m tot_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m: tot_answer,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m: true_answer,\n\u001b[1;32m     32\u001b[0m })\n",
      "Cell \u001b[0;32mIn[344], line 124\u001b[0m, in \u001b[0;36mTreeOfThoughts.solve\u001b[0;34m(self, question, max_steps, n_samples_per_step, k_best_thoughts)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Evaluate candidates\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m candidate_scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_thought(question, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthought\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()) \n\u001b[1;32m    125\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Select and accumulate best candidates\u001b[39;00m\n\u001b[1;32m    128\u001b[0m best_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_best_thoughts(candidates, candidate_scores, k_best_thoughts)\n",
      "Cell \u001b[0;32mIn[344], line 124\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Evaluate candidates\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m candidate_scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_thought\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mthought\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    125\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Select and accumulate best candidates\u001b[39;00m\n\u001b[1;32m    128\u001b[0m best_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_best_thoughts(candidates, candidate_scores, k_best_thoughts)\n",
      "Cell \u001b[0;32mIn[344], line 90\u001b[0m, in \u001b[0;36mTreeOfThoughts.evaluate_thought\u001b[0;34m(self, question, thought, cache)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_cache[thought]\n\u001b[1;32m     85\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mRate how likely this thought will lead to the correct answer (1-10):\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124mThought: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthought\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124mRating (1-10): \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m---> 90\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_with_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[344], line 17\u001b[0m, in \u001b[0;36mTreeOfThoughts.chat_with_gpt\u001b[0;34m(self, prompt, n, stop)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[0;32m---> 17\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     completions \u001b[38;5;241m=\u001b[39m [choice\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices]\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# tokens_used = sum(count_tokens(prompt + c, self.model) for c in completions)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# self.total_tokens += tokens_used\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/openai/resources/chat/completions.py:850\u001b[0m, in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/openai/_base_client.py:996\u001b[0m, in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/ssl.py:1260\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1257\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1258\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1259\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/nndl/lib/python3.9/ssl.py:1135\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. load DeepInfra api key\n",
    "api_key = os.getenv(\"DEEPINFRA_TOKEN\", \"\")\n",
    "\n",
    "# 2. Load and sample dataset\n",
    "dataset = load_dataset(\"cs5260_val_random300.jsonl\", sample_size=25)\n",
    "\n",
    "# 3. Initialize reasoning solvers\n",
    "tot_solver = TreeOfThoughts(api_key)\n",
    "cot_solver = ChainOfThought(api_key)\n",
    "\n",
    "# 4. process dataset\n",
    "tot_results, cot_results = [], []\n",
    "tot_tokens, cot_tokens = 0, 0\n",
    "\n",
    "print(\"\\nProcessing questions...\")\n",
    "for item in tqdm(dataset):\n",
    "    question = item[\"question\"]\n",
    "    true_answer = item[\"answer\"]\n",
    "    \n",
    "    # ToT solving\n",
    "    tot_solution = tot_solver.solve(\n",
    "        question=question,\n",
    "        max_steps=4, \n",
    "        n_samples_per_step=3, \n",
    "        k_best_thoughts=2 \n",
    "    )\n",
    "    tot_answer = extract_value(tot_solution)\n",
    "    tot_results.append({\n",
    "        \"question_id\": item[\"question_id\"],\n",
    "        \"predicted\": tot_answer,\n",
    "        \"true\": true_answer,\n",
    "    })\n",
    "    \n",
    "    # CoT solving\n",
    "    cot_solution, tokens = cot_solver.solve(question)\n",
    "    cot_answer = extract_value(cot_solution)\n",
    "    cot_results.append({\n",
    "        \"question_id\": item[\"question_id\"],\n",
    "        \"predicted\": cot_answer,\n",
    "        \"true\": true_answer,\n",
    "    })\n",
    "    cot_tokens += tokens\n",
    "\n",
    "# Calculate metrics\n",
    "tot_accuracy = evaluate_accuracy([r[\"predicted\"] for r in tot_results], \n",
    "                                [r[\"true\"] for r in tot_results])\n",
    "cot_accuracy = evaluate_accuracy([r[\"predicted\"] for r in cot_results], \n",
    "                                [r[\"true\"] for r in cot_results])\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Validation Results ===\")\n",
    "print(f\"ToT Accuracy: {tot_accuracy:.2%}\")\n",
    "print(f\"CoT Accuracy: {cot_accuracy:.2%}\")\n",
    "print(f\"ToT Total Tokens: {tot_solver.total_tokens}\")\n",
    "print(f\"CoT Total Tokens: {cot_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "- Refer to [here](https://www.kaggle.com/competitions/cs-5260-spring-2025-assignment-1/data) for submission format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate on test set and build submission file.\n",
    "# 1. load DeepInfra api key\n",
    "api_key = os.getenv(\"DEEPINFRA_TOKEN\", \"\")\n",
    "\n",
    "# 2. Load and sample dataset\n",
    "dataset = load_dataset(\"cs5260_val_random300.jsonl\", sample_size=3)\n",
    "\n",
    "# 3. Initialize reasoning solvers\n",
    "tot_solver = TreeOfThoughts(api_key)\n",
    "\n",
    "tot_results = []\n",
    "\n",
    "print(\"\\nProcessing questions...\")\n",
    "for item in tqdm(dataset):\n",
    "    question = item[\"question\"]\n",
    "    question_id = item[\"question_id\"]\n",
    "\n",
    "    tot_solution = tot_solver.solve(\n",
    "        question=question,\n",
    "        max_steps=8, \n",
    "        n_samples_per_step=3, \n",
    "        k_best_thoughts=2 \n",
    "    )\n",
    "    tot_answer = extract_value(tot_solution)\n",
    "    tot_results.append({\n",
    "        \"question_id\": item[\"question_id\"],\n",
    "        \"predicted\": tot_answer,\n",
    "    })\n",
    "\n",
    "import csv\n",
    "with open('A0297803U.csv', 'w', newline='') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"question_id\", \"answer\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(tot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [10:46<00:00, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation Results ===\n",
      "ToT Accuracy: 84.00%\n",
      "ToT Total Tokens: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_item(item, api_key):\n",
    "    \"\"\"Thread worker function to process one question with both solvers\"\"\"\n",
    "    # Initialize fresh solvers for thread safety\n",
    "    tot_solver = TreeOfThoughts(api_key)\n",
    "    # cot_solver = ChainOfThought(api_key)\n",
    "    \n",
    "    # ToT processing\n",
    "    tot_solution = tot_solver.solve(\n",
    "        question=item[\"question\"],\n",
    "        max_steps=8,\n",
    "        n_samples_per_step=4,\n",
    "        k_best_thoughts=4\n",
    "    )\n",
    "    \n",
    "    # CoT processing\n",
    "    # cot_solution, cot_tokens = cot_solver.solve(item[\"question\"])\n",
    "    \n",
    "    return {\n",
    "        \"tot_entry\": {\n",
    "            \"question_id\": item[\"question_id\"],\n",
    "            \"predicted\": extract_value(tot_solution),\n",
    "            \"true\": item[\"answer\"]\n",
    "        },\n",
    "        # \"cot_entry\": {\n",
    "        #     \"question_id\": item[\"question_id\"],\n",
    "        #     \"predicted\": extract_value(cot_solution),\n",
    "        #     \"true\": item[\"answer\"]\n",
    "        # },\n",
    "        \"tot_tokens\": tot_solver.total_tokens,\n",
    "        # \"cot_tokens\": cot_tokens\n",
    "    }\n",
    "\n",
    "# Main execution flow\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = os.getenv(\"DEEPINFRA_TOKEN\", \"\")\n",
    "    dataset = load_dataset(\"cs5260_val_random300.jsonl\", sample_size=50)\n",
    "    \n",
    "    tot_results, cot_results = [], []\n",
    "    tot_tokens_total, cot_tokens_total = 0, 0\n",
    "\n",
    "    print(\"\\nProcessing questions...\")\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_item, item, api_key) for item in dataset]\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            result = future.result()\n",
    "            tot_results.append(result[\"tot_entry\"])\n",
    "            # cot_results.append(result[\"cot_entry\"])\n",
    "            # tot_tokens_total += result[\"tot_tokens\"]\n",
    "            # cot_tokens_total += result[\"cot_tokens\"]\n",
    "\n",
    "    # Calculate and display metrics\n",
    "    tot_accuracy = evaluate_accuracy([r[\"predicted\"] for r in tot_results], \n",
    "                                    [r[\"true\"] for r in tot_results])\n",
    "    # cot_accuracy = evaluate_accuracy([r[\"predicted\"] for r in cot_results], \n",
    "    #                                 [r[\"true\"] for r in cot_results])\n",
    "\n",
    "    print(\"\\n=== Validation Results ===\")\n",
    "    print(f\"ToT Accuracy: {tot_accuracy:.2%}\")\n",
    "    # print(f\"CoT Accuracy: {cot_accuracy:.2%}\")\n",
    "    print(f\"ToT Total Tokens: {tot_tokens_total}\")\n",
    "    # print(f\"CoT Total Tokens: {cot_tokens_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [10:59<00:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. CSV file 'A0297803U.csv' created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#multithread for test set\n",
    "import os\n",
    "import csv\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_question(item, api_key):\n",
    "    \"\"\"Process a single question using TreeOfThoughts solver\"\"\"\n",
    "    # Create new solver instance for thread safety\n",
    "    tot_solver = TreeOfThoughts(api_key)\n",
    "    \n",
    "    tot_solution = tot_solver.solve(\n",
    "        question=item[\"question\"],\n",
    "        max_steps=5, \n",
    "        n_samples_per_step=3, \n",
    "        k_best_thoughts=2 \n",
    "    )\n",
    "    \n",
    "    tot_answer = extract_value(tot_solution)\n",
    "    return {\n",
    "        \"question_id\": item[\"question_id\"],\n",
    "        \"predicted\": tot_answer,\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # 1. Load DeepInfra API key\n",
    "    api_key = os.getenv(\"DEEPINFRA_TOKEN\", \"\")\n",
    "\n",
    "    # 2. Load and sample dataset\n",
    "    dataset = load_dataset(\"cs5260_test_random300.jsonl\", sample_size=300)\n",
    "\n",
    "    # 3. Process questions using ThreadPoolExecutor\n",
    "    tot_results = []\n",
    "    print(\"\\nProcessing questions...\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks for each question\n",
    "        futures = [executor.submit(process_question, item, api_key) \n",
    "                  for item in dataset]\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), \n",
    "                         total=len(futures)):\n",
    "            tot_results.append(future.result())\n",
    "\n",
    "    # 4. Create CSV file\n",
    "    with open('A0297803U_2.csv', 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"question_id\", \"predicted\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(tot_results)\n",
    "    \n",
    "    print(\"Processing complete. CSV file 'A0297803U.csv' created.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nndl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
